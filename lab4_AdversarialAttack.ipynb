{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Uuz2Qz9dsZzvTc1NuxElbhbqFFV_hsrk","timestamp":1744313181761}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **Adversarial Attacks Against LLM-Based Spam Filters**\n","\n","Adapted from EN.650.654 Computer Intrusion Detection\n","\n","Professor: Dr. Xiangyang Li https://isi.jhu.edu/faculty/xiangyang-li/\n","\n","TA: Yi He yhe106@jh.edu\n"],"metadata":{"id":"iKDcdwsGj0vi"}},{"cell_type":"markdown","source":["## **Introduction**\n","\n","Using the magic words to generate adversarial emails.\n","Evaluating the effectiveness of these attacks against large language model-based spam filters.\n","Analyzing the impact of different insertion positions on attack success rates."],"metadata":{"id":"8UI9JYAaVCWw"}},{"cell_type":"markdown","source":["## **1. Loading Dependencies**\n"],"metadata":{"id":"AcAxrnjuW4we"}},{"cell_type":"markdown","source":["Mount your drive\n"],"metadata":{"id":"OVgsDUUrwRLB"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"3gUz4NBgZrYP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["loading dependencies for bert model"],"metadata":{"id":"J_zpptvzwVcd"}},{"cell_type":"code","source":["import torch\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from transformers import BertTokenizer, BertForSequenceClassification, TrainingArguments, Trainer\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","import numpy as np\n","from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n","from tqdm import trange"],"metadata":{"id":"YaMrNKRFYdyQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **2. Loading & Spliting Dataset**\n","Split into 80% training and 20% validation for experiments.\n","  - **Training Dataset** will be used to train our spam filter\n","  - **Validation Dataset** will be used first to evaluate spam filter's performance, and then by inserting magic words, we get modified emails for testing attack succuss rate. this is used as **test set** in part 1\n","      - Reserve 10 random spam emails for modification from Validation dataset\n","\n","<font color='red'> Don't forget to replace the path to messages.csv</font>"],"metadata":{"id":"rh-KW3d8XdeN"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","def data_extraction():\n","\n","  # Change to the filename you uploaded.\n","  df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Gen/messages.csv')\n","\n","  x = df.message\n","  y = df.label\n","\n","  x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=99, stratify=y)\n","\n","  spam_emails = x_val[y_val == 1]\n","  reserved_samples = spam_emails.sample(n=10, random_state=2025)\n","\n","  reserved_samples.to_csv(\"reserved_samples.csv\", index=False, header=True)\n","\n","  return x_train, x_val, y_train, y_val, reserved_samples\n","\n","train_inputs, validation_inputs, train_labels, validation_labels, reserved_samples = data_extraction()\n","print(train_inputs.shape, validation_inputs.shape)\n","\n","# Display reserved samples\n","for i, email in enumerate(reserved_samples.tolist()):\n","    print(f\"Sample {i+1}: {email}\\n\")\n"],"metadata":{"id":"d_NPSNKqomkP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **3. Pre-Process Data**\n","In this section, we define the preprocessing steps.\n","- The text is tokenized, converted to token IDs, padded or truncated to a fixed length, and attention masks are generated.\n","\n","These preprocessing steps ensure that the data is ready for efficient input into each model."],"metadata":{"id":"zLcWQsGoczOf"}},{"cell_type":"code","source":["def preprocessing(input_text, tokenizer):\n","    '''\n","    Returns <class transformers.tokenization_utils_base.BatchEncoding>\n","    '''\n","    return tokenizer(\n","        input_text,\n","        truncation=True,\n","        padding='max_length',\n","        max_length=32,\n","        return_tensors='pt'\n","    )\n","\n","# Load the BERT tokenizer\n","bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","# Function to preprocess data for BERT\n","def preprocessing_for_bert(inputs, labels, tokenizer=bert_tokenizer):\n","    '''\n","    data: Pandas dataframe containing data and their labels.\n","    Returns list of 2D tensors.\n","    '''\n","    encoding_dict = preprocessing(inputs.tolist(), tokenizer)\n","    token_id = encoding_dict['input_ids']\n","    attention_masks = encoding_dict['attention_mask']\n","    labels = torch.tensor(labels.tolist())\n","    return token_id, attention_masks, labels\n","\n"],"metadata":{"id":"wZ-BIOvBzE0a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **4. Training LLM Spam Filters and Evaluating Adversarial Attack**\n","In this section, we train spam filters using BERT model.\n","\n","First, we train the model on a labeled spam dataset. The training process involves fine-tuning the pre-trained models to classify spam and non-spam messages.\n","\n","Once the models are trained, we save them to disk for future evaluation. In subsequent steps, we can load the saved models to evaluate their performance, including testing their robustness against adversarial attacks aimed at evading spam detection. This approach allows for efficient re-use of the trained models without retraining each time."],"metadata":{"id":"q8zBfr5QX_Fr"}},{"cell_type":"markdown","source":["### **Training Bert Spam Filter**"],"metadata":{"id":"uhUYyld6lCNp"}},{"cell_type":"code","source":["# preprocess the training dataset for bert\n","train_token_id, train_attention_masks, train_labels = preprocessing_for_bert(train_inputs, train_labels, bert_tokenizer)\n","# preprocess the validation dataset for bert\n","validation_token_id, validation_attention_masks, validation_labels = preprocessing_for_bert(validation_inputs, validation_labels, bert_tokenizer)\n","print(train_token_id.shape, validation_token_id.shape)\n","\n","# DataLoader initialization\n","batch_size = 16\n","train_data = TensorDataset(train_token_id, train_attention_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","validation_data = TensorDataset(validation_token_id, validation_attention_masks, validation_labels)\n","validation_sampler = SequentialSampler(validation_data)\n","validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"QlZx3uXy4G0I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the model\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n","model.to(device)\n","\n","# Training setup\n","optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n","\n","epochs = 2\n","for _ in trange(epochs, desc=\"Epoch\"):\n","    # Set model to training mode\n","    model.train()\n","    tr_loss = 0\n","    nb_tr_steps = 0\n","\n","    for step, batch in enumerate(train_dataloader):\n","        batch = tuple(t.to(device) for t in batch)\n","        b_input_ids, b_input_mask, b_labels = batch\n","\n","        # Clear out gradients\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        outputs = model(input_ids=b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n","        loss = outputs.loss\n","\n","        # Backward pass\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Update tracking variables\n","        tr_loss += loss.item()\n","        nb_tr_steps += 1\n","\n","    # ========== Validation ==========\n","\n","    # Set model to evaluation mode\n","    model.eval()\n","\n","    # Tracking variables\n","    all_labels = []\n","    all_preds = []\n","\n","    for batch in validation_dataloader:\n","        batch = tuple(t.to(device) for t in batch)\n","        b_input_ids, b_input_mask, b_labels = batch\n","\n","        with torch.no_grad():\n","            # Forward pass\n","            outputs = model(input_ids=b_input_ids, attention_mask=b_input_mask)\n","            logits = outputs.logits\n","            predicted_labels = torch.argmax(logits, dim=1)\n","\n","        all_labels.extend(b_labels.cpu().numpy())\n","        all_preds.extend(predicted_labels.cpu().numpy())\n","\n","    # Calculate evaluation metrics\n","    accuracy = accuracy_score(all_labels, all_preds)\n","    precision = precision_score(all_labels, all_preds, average=\"binary\", zero_division=1)\n","    recall = recall_score(all_labels, all_preds, average=\"binary\", zero_division=1)\n","    f1 = f1_score(all_labels, all_preds, average=\"binary\", zero_division=1)\n","\n","    # Calculate False Positive Rate (FPR) and False Negative Rate (FNR)\n","    tn, fp, fn, tp = confusion_matrix(all_labels, all_preds).ravel()\n","    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n","    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n","\n","    # Print metrics\n","    print(f\"Epoch {_ + 1}/{epochs}\")\n","    print(f\"Accuracy: {accuracy:.4f}\")\n","    print(f\"Precision: {precision:.4f}\")\n","    print(f\"Recall: {recall:.4f}\")\n","    print(f\"F1 Score: {f1:.4f}\")\n","    print(f\"False Positive Rate (FPR): {fpr:.4f}\")\n","    print(f\"False Negative Rate (FNR): {fnr:.4f}\")\n","    print(\"\\n\\t - Train loss: {:.4f}\".format(tr_loss / nb_tr_steps))\n","\n","    # save model\n","    model_path = f\"Bert_epoch{_ + 1}.pth\"\n","    torch.save(model.state_dict(), model_path)\n","    print(f\"Model saved to {model_path}\")\n"],"metadata":{"id":"1UYTSRv_bUlA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **5.Adversarial Attack with Magic Words**\n","Apply different insertion strategies to the reserved 10 spam emails to find out the attack success rate. You can manually do the insertion or write some code to do it.\n","\n","- word based insertion, insert your magic words as a string.\n","  - word_0: insert at the begining of the email\n","  - word_1: insert behind the first sentence\n","  - word_2: insert behind the second sentence\n","  - word_3: insert behind the third sentence\n","  - word_∞: insert at the end position\n","- sentence based insertion, insert your magic sentences (You need to create one or two semantically meaningful sentences from these words.)\n","  - sentence_0: insert at the begining of the email\n","  - sentence_1: insert behind the first sentence\n","  - sentence_2: insert behind the second sentence\n","  - sentence_3: insert behind the third sentence\n","  - sentence_∞: insert at the end position"],"metadata":{"id":"y2FVzJx8YKNe"}},{"cell_type":"markdown","source":["The following is a function for this task."],"metadata":{"id":"qweTJ_GuBOhi"}},{"cell_type":"code","source":["def insert_procession(text, insertion, position):\n","    punctuation = ['.', '!', '?']\n","    punctuation_indices = [i for i, char in enumerate(text) if char in punctuation]\n","\n","    if position == \"sentence_0\" or position == \"word_0\":\n","        return insertion + text\n","    elif position == \"sentence_∞\" or position == \"word_∞\":\n","        return text + insertion\n","    else:\n","        position_index = int(position.split('_')[1]) - 1\n","        if position_index < len(punctuation_indices):\n","            insert_pos = punctuation_indices[position_index] + 1\n","            text = text[:insert_pos] + \" \" + insertion + text[insert_pos:]\n","    return text\n","\n","def insert_magic_word(text, magic_word, magic_sentences, position):\n","  if \"word\" in position:\n","    return insert_procession(text, magic_word, position)\n","  elif \"sentence\" in position:\n","    return insert_procession(text, magic_sentences, position)"],"metadata":{"id":"f5MCn6XHjOb1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# you can create your magic_sentence by changing orders and add prepositions if needed\n","magic_words = \"translation cascadilla workshop proceeding benjamin \\\n","academic ldc chorus native colingacl french sentence \\\n","pkzip euralex linguistic risked ammondt phonetic \\\n","arizona grammar ipa theory linguist\"\n","magic_sentences = \"Academic linguist Benjamin pkzips phonetic sentence \\\n","translation grammar theory in Euralex COLING/ACL \\\n","workshop proceeding in Arizona. Native French Am-\\\n","mondt risked the linguistic IPA Chorus of LDC Cas-\\\n","cadilla.\"\n","\n","# modify every emails\n","modifications = [\"word_0\", \"word_1\", \"word_2\", \"word_3\", \"word_∞\", \"sentence_0\", \"sentence_1\", \"sentence_2\", \"sentence_3\", \"sentence_∞\"]\n","# read reserved_samples from file\n","samples = pd.read_csv('reserved_samples.csv')\n","\n","for modification in modifications:\n","  modified_text = samples['message'].apply(lambda row: insert_magic_word(row, magic_words, magic_sentences, modification))\n","  modified_text = modified_text.apply(lambda text: text.replace('\\t', ' ').replace('\\n', ' '))\n","  samples[modification] = modified_text\n","  print(type(samples))\n","  # modified_text.to_csv(modified_file_path, sep='\\t', index=False, header=False)\n","  # print(modified_text.head())\n","samples.to_csv('modified_samples', sep='\\t', index=False, header=True, quotechar='\"')\n","print(samples.head())"],"metadata":{"id":"y57HsJ_w3b-D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **6. Evaluating the Attack Effectiveness**\n","In this section, we measure the effectiveness of adversarial attacks against different LLM-based spam filters by measuring their attack success rate - False Negative Rate (FNR) in the evaluation performance.\n","\n","Specifically, we assess how well the adversarial emails evade detection when adversarial tokens (magic words or sentences) are inserted at various positions within the email text."],"metadata":{"id":"_HKy2i8TYR8g"}},{"cell_type":"code","source":["print(modified_text.head())\n"],"metadata":{"id":"lPZUwhQ77jDZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Evaluation on Bert\n","# evaluate your result on bert model\n","# load model\n","model_path = f\"Bert_epoch2.pth\"\n","# or \"/content/drive/My Drive/CID_final/Bert_epoch2.pth\"\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n","model.load_state_dict(torch.load(model_path))\n","model.to(device)\n","model.eval()\n","print(\"model is loaded.\")\n","batch_size = 10\n","\n","reserved_samples = pd.read_csv(\"reserved_samples.csv\")\n","labels = pd.Series([1]*10) # all \"1\" spam\n","reserved_token_id, reserved_attention_masks, reserved_labels = preprocessing_for_bert(reserved_samples['message'], labels, bert_tokenizer)\n","reserved_data = TensorDataset(reserved_token_id, reserved_attention_masks, reserved_labels)\n","reserved_sampler = SequentialSampler(reserved_data)\n","reserved_dataloader = DataLoader(reserved_data, sampler=reserved_sampler, batch_size=batch_size)\n","\n","all_preds = []\n","all_labels = [] # should be all \"1\" spam\n","for batch in reserved_dataloader:\n","    batch = tuple(t.to(device) for t in batch)\n","    b_input_ids, b_input_mask, b_labels = batch\n","    with torch.no_grad():\n","        outputs = model(input_ids=b_input_ids, attention_mask=b_input_mask)\n","        logits = outputs.logits\n","        predicted_labels = torch.argmax(logits, dim=1)\n","    all_preds.extend(predicted_labels.cpu().numpy())\n","    all_labels.extend(b_labels.cpu().numpy())\n","\n","accuracy = accuracy_score(all_labels, all_preds)\n","precision = precision_score(all_labels, all_preds, average=\"binary\", zero_division=1)\n","recall = recall_score(all_labels, all_preds, average=\"binary\", zero_division=1)\n","f1 = f1_score(all_labels, all_preds, average=\"binary\", zero_division=1)\n","\n","# Calculate False Positive Rate (FPR) and False Negative Rate (FNR)\n","tn, fp, fn, tp = confusion_matrix(all_labels, all_preds).ravel()\n","fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n","fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n","\n","# Print metrics\n","print(f\"Before modification\")\n","print(f\"Accuracy: {accuracy:.4f}\")\n","print(f\"Precision: {precision:.4f}\")\n","print(f\"Recall: {recall:.4f}\")\n","print(f\"F1 Score: {f1:.4f}\")\n","print(f\"False Positive Rate (FPR): {fpr:.4f}\")\n","print(f\"False Negative Rate (FNR): {fnr:.4f}\")\n","\n","modified_samples = pd.read_csv('modified_samples', sep='\\t')\n","\n","for modification in modifications:\n","  modified_text = modified_samples[modification]\n","  # preprocess data\n","  modified_token_id, modified_attention_masks, modified_labels = preprocessing_for_bert(modified_text, labels, bert_tokenizer)\n","  # use model to predict\n","  modified_data = TensorDataset(modified_token_id, modified_attention_masks, modified_labels)\n","  modified_sampler = SequentialSampler(modified_data)\n","  modified_dataloader = DataLoader(modified_data, sampler=modified_sampler, batch_size=batch_size)\n","\n","  all_preds = []\n","\n","  for batch in modified_dataloader:\n","      batch = tuple(t.to(device) for t in batch)\n","      b_input_ids, b_input_mask, b_labels = batch\n","      with torch.no_grad():\n","          outputs = model(input_ids=b_input_ids, attention_mask=b_input_mask)\n","          logits = outputs.logits\n","          predicted_labels = torch.argmax(logits, dim=1)\n","      all_preds.extend(predicted_labels.cpu().numpy())\n","  accuracy = accuracy_score(all_labels, all_preds)\n","  precision = precision_score(all_labels, all_preds, average=\"binary\", zero_division=1)\n","  recall = recall_score(all_labels, all_preds, average=\"binary\", zero_division=1)\n","  f1 = f1_score(all_labels, all_preds, average=\"binary\", zero_division=1)\n","\n","  # Calculate False Positive Rate (FPR) and False Negative Rate (FNR)\n","  tn, fp, fn, tp = confusion_matrix(all_labels, all_preds).ravel()\n","  fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n","  fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n","\n","  # Print metrics\n","  print(f\"After modification: {modification}\")\n","  print(f\"Accuracy: {accuracy:.4f}\")\n","  print(f\"Precision: {precision:.4f}\")\n","  print(f\"Recall: {recall:.4f}\")\n","  print(f\"F1 Score: {f1:.4f}\")\n","  print(f\"False Positive Rate (FPR): {fpr:.4f}\")\n","  print(f\"False Negative Rate (FNR): {fnr:.4f}\")\n","\n"],"metadata":{"id":"jiDxKtKuiZFx"},"execution_count":null,"outputs":[]}]}
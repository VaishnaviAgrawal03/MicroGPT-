{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"11jDVS-K4Vb-c3ryK7cxu5JQ_u1_KLRlE","timestamp":1744990622198}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Introduction\n","\n","This HR chatbot is powered by Retrieval-Augmented Generation (RAG) technology, enabling efficient analysis of various input formats such as .pdf, .doc, .docx and .png, .jpg, .jpeg. It is designed to assist in identifying the most suitable candidates for specific roles by leveraging document-based data."],"metadata":{"id":"ogLMR65pRKyw"}},{"cell_type":"markdown","source":["# Preparation\n","\n","\n","1.  Install necessary packages and mount Google Drive data\n","\n","2.  <font color='red'>Create a folder called \"RAG\" in your Google Drive, and upload the resumes folder </font>\n","\n","3.  Prepare for openai key and folder id\n"],"metadata":{"id":"BHqmYPriuZP-"}},{"cell_type":"code","source":["## Mount Google Drive Data (If using Google Colaboratory)\n","try:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","except:\n","    print(\"Mounting Failed.\")"],"metadata":{"id":"4a09u8dbeSSr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install pymupdf python-docx docx2txt pytesseract pillow\n","!pip install -q PyPDF2 nltk faiss-cpu openai langchain langchain_openai langchain_community tqdm fitz\n","!sudo apt install tesseract-ocr\n","# to solve some version conflicts of pymupdf\n","!pip uninstall pymupdf --yes\n","!pip install pymupdf\n","\n","import glob\n","import os\n","import sys\n","import io\n","import pytesseract\n","from PIL import Image\n","import textwrap\n","from PyPDF2 import PdfReader\n","from nltk.tokenize import sent_tokenize\n","import nltk\n","import pickle\n","import warnings\n","from tqdm import tqdm\n","from langchain_community.vectorstores import FAISS\n","from langchain_openai import OpenAIEmbeddings\n","from langchain_openai import ChatOpenAI\n","from langchain.chains import RetrievalQA\n","import pickle as pkl\n","import fitz  # PyMuPDF\n","import docx\n","import docx2txt\n","warnings.filterwarnings(\"ignore\", category=UserWarning)\n","nltk.download('punkt')"],"metadata":{"id":"XRmuKEAK0cUb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# replace with your openai key.\n","\n","#################\n","openai_key = \"YOUR OPENAI KEY\" ## Put your Open_AI Key here\n","#################\n","os.environ[\"OPENAI_API_KEY\"] = openai_key\n"],"metadata":{"id":"4R39jXbFdg3R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# extract text from pdf/doc/docx"],"metadata":{"id":"D8a4RUmiAtrf"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"-rPZAgzkk0L_"},"outputs":[],"source":["import nltk\n","nltk.download('punkt_tab')\n","# set the path\n","filepaths = glob.glob(\"/content/drive/MyDrive/RAG/Resumes/*.*\")\n","\n","\n","def extract_text_from_image(filepath):\n","    image = Image.open(filepath)\n","    text = pytesseract.image_to_string(image)\n","    return text\n","\n","def extract_text_from_pdf(filepath):\n","    pdf_reader = fitz.open(filepath)\n","    text = ''\n","    for page in pdf_reader:\n","        text += page.get_text()\n","    pdf_reader.close()\n","    return text\n","\n","def extract_text_from_doc(filepath):\n","    doc = docx.Document(filepath)\n","    text = '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n","    return text\n","\n","def extract_text_from_docx(filepath):\n","    text = docx2txt.process(filepath)\n","    return text\n","\n","def load_from_text(fstarter: str, text: str, split_size: int, overlap: int):\n","    text = sent_tokenize(text)\n","    i = 0\n","    data = []\n","    while i < len(text):\n","        splits = [fstarter]\n","        splits.extend(text[i:i+split_size])\n","        data.append(\" \".join(splits))\n","        i = i + (split_size - overlap)\n","    return data\n","\n","chunked_dataset = {}\n","split_size = 15\n","overlap = 1\n","\n","print(f\"{filepaths}\")\n","print(f\"Number of files: {len(filepaths)}\")\n","\n","i = 0\n","\n","for filepath in tqdm(filepaths):\n","    file_name = os.path.basename(filepath)\n","    i += 1\n","    extension = os.path.splitext(filepath)[1].lower()\n","\n","    if extension in ['.pdf', '.doc', '.docx', '.png', '.jpg', '.jpeg']:\n","        if extension == '.pdf':\n","            data = extract_text_from_pdf(filepath)\n","        elif extension == '.doc':\n","            data = extract_text_from_doc(filepath)\n","        elif extension == '.docx':\n","            data = extract_text_from_docx(filepath)\n","        elif extension in ['.png', '.jpg', '.jpeg']:\n","            data = extract_text_from_image(filepath)\n","        else:\n","            continue\n","\n","        fstarter = f\"Information from {file_name}, the {i}th document.\"\n","        chunked_data = load_from_text(fstarter, data, split_size, overlap)\n","        chunked_dataset[i] = chunked_data\n","        print(f\"Processed {file_name}, the {i}th document\")\n","        print(f\"Number of chunks: {len(chunked_data)}\")\n"]},{"cell_type":"markdown","source":["# Based on chunked data build local vector database"],"metadata":{"id":"QIlbh7Vau2o4"}},{"cell_type":"code","source":["embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n","\n","for doc_number in tqdm(chunked_dataset.keys()):\n","    folder_path = f\"/content/drive/MyDrive/RAG/Resumes_vectordatabase/{doc_number}\"\n","    if not os.path.exists(folder_path):\n","        vectorstore = FAISS.from_texts(chunked_dataset[doc_number], embedding=embeddings)\n","        vectorstore.save_local(folder_path=folder_path, index_name=\"Resumes\")\n"],"metadata":{"id":"3fvd0inLmXGu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ordinal = list(chunked_dataset.keys())\n","\n","base_folder_path = \"/content/drive/MyDrive/RAG/Resumes_vectordatabase\"\n","save_folder_path = \"/content/drive/MyDrive/RAG/Vectorstores\"\n","\n","vectorstore = FAISS.load_local(\n","    folder_path=f\"{base_folder_path}/{ordinal[0]}\",\n","    embeddings=embeddings,\n","    index_name=\"Resumes\",\n","    allow_dangerous_deserialization=True\n",")\n","\n","for i in range(1, len(ordinal)):\n","    _vectorstore = FAISS.load_local(\n","        folder_path=f\"{base_folder_path}/{ordinal[i]}\",\n","        embeddings=embeddings,\n","        index_name=\"Resumes\",\n","        allow_dangerous_deserialization=True\n","    )\n","    vectorstore.merge_from(_vectorstore)\n","\n","vectorstore.save_local(folder_path=save_folder_path, index_name=\"Resumes\")"],"metadata":{"id":"6ZpO3BOErN9u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["-----\n"],"metadata":{"id":"YibGYGmpsjSv"}},{"cell_type":"markdown","source":["# Ask llm questions\n","\n"],"metadata":{"id":"BI75BWstuuD5"}},{"cell_type":"code","source":["top_k_docs = 10\n","\n","persona = \"You are an HR manager. Answer my question concisely and reasonably. \\\n","If there are fewer than expected or no suitable candidates, \\\n","clearly state that there are not enough candidates available without offering unrelated options. \\\n","Only list candidates directly related to the specified field. \\\n","Do not include candidates from unrelated expertise or specialties, as experience in other specialties is irrelevant. \\\n"," If no suitable candidates are available, state this directly. \\\n"," If multiple candidates are listed, provide a brief summary of the best candidate after listing them.\"\n","\n","\n","llm = ChatOpenAI(model= \"gpt-4\", temperature=0.7, max_retries = 10) ### Change it as you wish\n","\n","embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n","\n","vectorstore = FAISS.load_local(folder_path=f\"/content/drive/MyDrive/RAG/Vectorstores\",\n","                                  embeddings=embeddings,\n","                                  index_name=\"Resumes\",\n","                                  allow_dangerous_deserialization=True)\n","\n","\n","retriever = vectorstore.as_retriever(search_type = \"similarity\", search_kwargs={\"k\":top_k_docs})\n","qa = RetrievalQA.from_chain_type(\n","        llm = llm,\n","        chain_type = \"stuff\",\n","        retriever = retriever,\n","        verbose = True,\n","        return_source_documents = True\n","    )\n","\n"],"metadata":{"id":"3NKjTikrskYW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query_history = \"\"\n","\n","while True:\n","    query = input('Enter your query: (type \"exit\" to exit) \\n')\n","\n","    if query.strip().lower() == \"exit\":\n","        print(\"Answer from AI HR manager: \\n\")\n","        print(\"goodbye\")\n","        break\n","\n","    print(\"\\n \\033[1mloading... \\033[0m\\n\")\n","\n","    original_stdout = sys.stdout\n","    captured_output = io.StringIO()\n","    sys.stdout = captured_output\n","\n","    sources = vectorstore.similarity_search_with_relevance_scores(query, k=5)\n","\n","    result = qa.invoke({\n","        \"persona\": persona,\n","        \"query\": \"<history>\" +query_history+\"</history> \\n <query>\"+query+\"</query>\",\n","    })\n","\n","    sys.stdout = original_stdout\n","\n","    output = captured_output.getvalue()\n","    filtered_output = output.replace(\"Entering new RetrievalQA chain...\\n\", \"\").replace(\"Finished chain.\\n\", \"\").replace(\"loading...\\n\", \"\")\n","\n","\n","    query_history += f\"User: {query}\\n\"\n","    query_history += f\"Bot: {result['result']}\\n\"\n","\n","    formatted_response = textwrap.dedent(result['result'])\n","    wrapped_string = textwrap.fill(formatted_response, width=60)\n","\n","    print(\"Answer from AI HR manager:\")\n","    print(wrapped_string)\n","    print(\"\\n\\n\")"],"metadata":{"id":"mJ8M5xxg7OdR"},"execution_count":null,"outputs":[]}]}